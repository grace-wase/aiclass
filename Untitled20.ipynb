{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCbgI8HSqfqJAeKu0sW2lP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grace-wase/aiclass/blob/main/Untitled20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai -q\n",
        "!pip install langchain_community -q\n",
        "!pip install langchain-experimental -q"
      ],
      "metadata": {
        "id": "27aMFzuTklCf"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "TsSAXFrAc7iC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "class ZhipuAI_embeddings:\n",
        "   def __init__(self, model_name:str=\"embedding-3\"):\n",
        "       self.model_name = model_name\n",
        "       self.base_url = \"https://open.bigmodel.cn/api/pass/v4\"\n",
        "       self.embedding = self.__init__model()\n",
        "\n",
        "   def __init__model(self) -> OpenAIEmbeddings:\n",
        "          return OpenAIEmbeddings(\n",
        "              model=self.model_name,\n",
        "              base_url=self.base_url,\n",
        "              api_key=userdata.get(\"Apikey\")\n",
        "          )\n",
        "\n",
        "embeddings_instance = ZhipuAI_embeddings()\n",
        "embeddings = embeddings_instance.embedding"
      ],
      "metadata": {
        "id": "Sj1BkCp2pXB_"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import ChatZhipuAI\n",
        "from google.colab import userdata\n",
        "\n",
        "students = ChatZhipuAI(\n",
        "    base_url=\"https://open.bigmodel.cn/paas/v4\",\n",
        "    api_key = userdata.get(\"Apikey\"),\n",
        "    model=\"glm-4.5\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZqZ1q70_xOQW"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def summarize_notes(text:str):\n",
        "  \"\"\"Takes student notes (a paragraph)\"\"\"\n",
        "  return f\"summary of the notes:{text[:2-3]}...\""
      ],
      "metadata": {
        "id": "xV_k4vpBVW9x"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def define_term(term:str):\n",
        "  \"\"\"return a one-sentence definition of a given AI term\n",
        "  (e.g.,\"transformer\",\"Embedding\")\"\"\"\n",
        "  definition={\n",
        "      \"transformer\":\"\"\"transformer is a type of Neural network architecture introduced in the paper\"\"\",\n",
        "      \"embeddings\":\"\"\"Embedding is a dense vector representation of the word\"\"\"\n",
        "\n",
        "  }\n",
        "  return f\"definition of(term):...\""
      ],
      "metadata": {
        "id": "obKMHV1WW1x6"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A_sCcFNee9g",
        "outputId": "1503e707-6c21-4b56-938a-2b0c5f0d505f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today? üòä', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 8, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 21}, 'model_name': 'glm-4.5', 'finish_reason': 'stop'}, id='run--0b0d7849-6310-45c8-8b5f-40be955aa169-0')"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "students.invoke([HumanMessage(content=\"hello\")])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_community.document_loaders.text import TextLoader\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader, PyPDFLoader\n",
        "def doc_parsing(file_path) -> list[Document]:\n",
        "  if file_path.endswith(\"pdf\"):\n",
        "    doc = PyPDFLoader(file_path=file_path)\n",
        "    doc = doc.load()\n",
        "    semantic_splitter = SemanticChunker(\n",
        "        embeddings=embeddings,\n",
        "        breakpoint_threshold_type=\"percentile\",\n",
        "        breakpoint_threshold_amount =89\n",
        "\n",
        "    )\n",
        "    full_text = doc[0].page_content if doc else\"\"\n",
        "    if not full_text:\n",
        "      return[]\n",
        "    raw_chunks = semantic_splitter.split_text(full_text)\n",
        "    print(f\"the number of chucks:{len(raw_chunks)}\")\n",
        "    docs=[Document(page_content=chunk,metadata=doc[0].metadata)for chunk in raw_chunks]\n",
        "    return docs\n",
        "  elif file_path.endswith(\".txt\"):\n",
        "    doc=TextLoader(file_path=file_path)\n",
        "    doc=doc.load()\n",
        "    semantic_splitter=SemanticChunker(\n",
        "        embeddings=embeddings,\n",
        "        breakpoint_threshold_type=\"percentile\",\n",
        "        breakpoint_threshold_amount=89\n",
        "    )\n",
        "    full_text=doc[0].page_content if doc else \"\"\n",
        "    if not full_text:\n",
        "      return[]\n",
        "    raw_chunks=semantic_splitter.splitter.split_text(full_text)\n",
        "    docs=[Document(page_content=chunk,metadata=doc[0].metadata)for chunk in raw_chunks]\n",
        "    return docs\n",
        "  else:\n",
        "       return[]"
      ],
      "metadata": {
        "id": "ED52P8ru0viu"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_template = \"\"\"\n",
        "Your name is AICLASS assistant a smart, yet sassy assistant working at takenolab, your have a better knowledge of takenolab operations,\n",
        "your task is to be supportive, provide proper guidance to students that are having troubles with the course content,\n",
        "you have to answer them direct and precise, if you dont have any advice to them dont generate any respose kindly\n",
        "tell them so.\n",
        "When you receive:\n",
        "‚Ä¢ question: the student‚Äôs problem\n",
        "‚Ä¢ context: optionally, the content of any uploaded documents\n",
        "\n",
        "If `context` is non-empty, you **must** use it to inform your answer. If you don‚Äôt have enough information\n",
        "from the question and context combined, tell the student you can‚Äôt help further.\n",
        "\n",
        "student problem:\n",
        "{question}\n",
        "\n",
        "uploaded document context (if any):\n",
        "{context}\n",
        "\n",
        "your smart advice or solution:\n",
        "\n",
        "\"\"\"\n",
        "new_template = \"\"\"\n",
        "Your name is AICLASS assistant, a smart, yet sassy assistant working at takenolab. You have a deep knowledge of takenolab operations.\n",
        "Your task is to be supportive, provide proper guidance to students who are having trouble with the course content.\n",
        "You must answer them directly and precisely. If you don't have any advice for them, kindly tell them so and do not generate any other response.\n",
        "\n",
        "When you receive:\n",
        "‚Ä¢ chat_history: the previous turns of the conversation (if any)\n",
        "‚Ä¢ question: the student‚Äôs current problem\n",
        "‚Ä¢ context: optionally, the content of any uploaded documents relevant to the current question\n",
        "\n",
        "If `context` is non-empty, you **must** use it to inform your answer. If you don‚Äôt have enough information\n",
        "from the question and context combined, tell the student you can‚Äôt help further.\n",
        "If `chat_history` is provided, use it to understand the full context of the current question.\n",
        "\n",
        "<chat_history>\n",
        "{chat_history}\n",
        "</chat_history>\n",
        "\n",
        "student problem:\n",
        "{question}\n",
        "\n",
        "uploaded document context (if any):\n",
        "{context}\n",
        "\n",
        "your smart advice or solution:\n",
        "\n",
        "\"\"\"\n",
        "SUB_QUERY_TEMPLATE = \"\"\"\n",
        "You are a helpful assistant that generates multiple search queries based on a single input query.\n",
        "Generate {num_queries} diverse search queries related to the user's question, which can be used to retrieve relevant documents.\n",
        "The queries should be concise and cover different aspects or angles of the original question.\n",
        "\n",
        "Original Question: {question}\n",
        "\n",
        "Generated Queries:\n",
        "-\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VyqcLYp8DriN"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "import gradio as gr\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.documents import Document\n",
        "from typing import TypedDict, List\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain_core.messages import AIMessage, HumanMessage, BaseMessage\n",
        "from typing import Optional,Tuple, Dict\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)\n",
        "def call_assistant(question:str=\"\", context_docs:List[Optional[Document]]=None,chat_history: List[Tuple[str, str]]=None):\n",
        "    prompt_template = PromptTemplate.from_template(new_template)\n",
        "    chain = prompt_template | students\n",
        "    history_str = \"\"\n",
        "    if chat_history:\n",
        "        for human_msg, ai_msg in chat_history:\n",
        "            history_str += f\"User: {human_msg}\\n Assistant: {ai_msg}\\n\"\n",
        "    context_text =\"\"\n",
        "    if context_docs:\n",
        "         context_text = \"\\n\\n\".join(d.page_content for d in context_docs)\n",
        "    response = chain.invoke({\n",
        "        \"question\": question,\n",
        "        \"context\": context_text,\n",
        "        \"chat_history\": history_str\n",
        "    })\n",
        "    return response.content\n",
        "def generate_sub_queries(original_question: str, num_queries: int = 3) -> List[str]:\n",
        "    sub_query_prompt = PromptTemplate.from_template(template=SUB_QUERY_TEMPLATE)\n",
        "    sub_query_chain = sub_query_prompt | students\n",
        "\n",
        "    response = sub_query_chain.invoke({\n",
        "        \"question\": original_question,\n",
        "        \"num_queries\": num_queries\n",
        "    })\n",
        "\n",
        "    queries = [q.strip() for q in response.content.split('-') if q.strip()]\n",
        "    print(f\"Generated sub-queries: {queries}\")\n",
        "    return queries\n",
        "def retrieve_and_answer_with_history(question: str, chat_history: List[Dict])->str:\n",
        "    # retrieved = vector_store.similarity_search(question)\n",
        "    formatted_chat_history_for_llm = []\n",
        "    for msg in chat_history:\n",
        "        if msg[\"role\"] == \"user\":\n",
        "            current_user_msg = msg[\"content\"]\n",
        "        elif msg[\"role\"] == \"assistant\":\n",
        "            formatted_chat_history_for_llm.append((current_user_msg, msg[\"content\"]))\n",
        "            current_user_msg = None\n",
        "    if len(vector_store.store.items()) >= 0:\n",
        "        transformed_queries = generate_sub_queries(question, num_queries=3)\n",
        "        all_retrieved_docs = []\n",
        "        seen_doc_contents = set()\n",
        "        for query in transformed_queries:\n",
        "            retrieved_for_query = vector_store.similarity_search(query)\n",
        "            for doc in retrieved_for_query:\n",
        "                if doc.page_content not in seen_doc_contents:\n",
        "                    all_retrieved_docs.append(doc)\n",
        "                    seen_doc_contents.add(doc.page_content)\n",
        "        ai_response = call_assistant(question, context_docs=all_retrieved_docs, chat_history=formatted_chat_history_for_llm)\n",
        "        return ai_response\n",
        "    ai_response = call_assistant(question,chat_history=formatted_chat_history_for_llm)\n",
        "    return ai_response\n",
        "def doc_loader(file_path):\n",
        "    docs = doc_parsing(file_path)\n",
        "    if not docs:\n",
        "        return \"No content found or processed in the document.\"\n",
        "    _ = vector_store.add_documents(documents=docs)\n",
        "    return docs[0].page_content[:200] + \"...\"\n",
        "\n",
        "def interface():\n",
        "    iface = gr.ChatInterface(\n",
        "        fn=retrieve_and_answer_with_history,\n",
        "        chatbot=gr.Chatbot(height=200, type='messages', label=\"Assistant\"),\n",
        "        textbox=gr.Textbox(lines=2,submit_btn=True ),\n",
        "        title=\"Takenolab AIClass Assistant (Conversational RAG)\",\n",
        "        type='messages',\n",
        "        description=\"Ask a question about your course content and get smart advice, supporting multi-turn conversations.\",\n",
        "    )\n",
        "    docs_interface = gr.Interface(\n",
        "        fn=doc_loader,\n",
        "        inputs=gr.File(label=\"Choose a file to upload\",\n",
        "                       type='filepath',\n",
        "                       file_count='single',\n",
        "                       show_label=True\n",
        "                       ),\n",
        "        description=\"Upload a document to run retrieval‚Äêaugmented generation.\",\n",
        "        outputs=gr.TextArea()\n",
        "    )\n",
        "    table = gr.TabbedInterface(\n",
        "        [iface,docs_interface],\n",
        "        tab_names= ['Chat', \"Upload File for RAG\"],\n",
        "        title=\"LLM, RAG AND PROMPTS, Text Generation\"\n",
        "    )\n",
        "    table.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "5SCFy5FQSlF6"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interface()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-xWZwPV0br96",
        "outputId": "7fe9f464-2248-4d75-9f32-444d85ef144b"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://d02ae22583b1f9bab6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d02ae22583b1f9bab6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 667, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 349, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2274, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1781, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 915, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1614308196.py\", line 63, in doc_loader\n",
            "    docs = doc_parsing(file_path)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1392124858.py\", line 18, in doc_parsing\n",
            "    raw_chunks = semantic_splitter.split_text(full_text)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_experimental/text_splitter.py\", line 228, in split_text\n",
            "    distances, sentences = self._calculate_sentence_distances(single_sentences_list)\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_experimental/text_splitter.py\", line 203, in _calculate_sentence_distances\n",
            "    embeddings = self.embeddings.embed_documents(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/embeddings/base.py\", line 591, in embed_documents\n",
            "    return self._get_len_safe_embeddings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_openai/embeddings/base.py\", line 479, in _get_len_safe_embeddings\n",
            "    response = self.client.create(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/embeddings.py\", line 132, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.InternalServerError: Error code: 500 - {'error': {'code': '500', 'message': '404 NOT_FOUND'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://d02ae22583b1f9bab6.gradio.live\n"
          ]
        }
      ]
    }
  ]
}